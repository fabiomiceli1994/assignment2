\documentclass[11pt]{article}
%\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
%\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
%\usepackage{graphicx} % Required for the inclusion of images
%\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage[margin=1in]{geometry}
\usepackage{microtype}
\usepackage[english]{babel}
\usepackage[utf8]{inputenx}
\usepackage{float}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{makeidx}
\usepackage{hyperref}
\usepackage{braket}
%\usepackage[latin1]{inputenc}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{fullpage}
\usepackage{hyphenat}
\usepackage{float}
\usepackage{longtable}
\usepackage{picture}
\usepackage{multicol}
\usepackage{fancyhdr}
\usepackage{wrapfig}
\usepackage{geometry}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}

\theoremstyle{definition}
\newtheorem{definition}{Definition}



%\usepackage{tikz}
%\usetikzlibrary{graphs}
%\usepackage{pgfplots}
%\pgfplotsset{compat=newest}

\setlength\parindent{0pt} % Removes all indentation from paragraphs

\renewcommand{\figurename}{Grafico}
\renewcommand{\tablename}{Tabella}
\newcommand{\at}[2][]{#1\Big|_{#2}}
\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{C1 - Assignment 2 Report: Advection-Diffusion-Reaction equation.} % Title

\author{Student Number: 1894945} % Author name

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date

\begin{center}
C1 - Assignment 2 Report \hfill
Student Number: 1894945
\vspace{3pt} \hrule \vspace{3pt} \hrule
\end{center}

%\clearpage
\tableofcontents

\clearpage
% If you wish to include an abstract, uncomment the lines below
%\begin{abstract}


%\end{abstract}
%\clearpage 

\section{Introduction}

\subsection{Boundary value problem}

\begin{definition}
	\label{defn:BVP}
	Let $\Omega\in\mathbb{R}^{d}$, for $d=\lbrace 1, 2\rbrace$ be a bounded, simply connected, open domain. The Boundary Value Problem (BVP) that will be considered in the following is find, for $f$ and $v$ given, a function $u$ such that 
	\begin{equation}
		\label{eqn:BVP}
		\mathcal{L}[u]=f\hspace{3mm}in\hspace{1mm}\Omega,\hspace{15mm}u=v\hspace{3mm}on\hspace{1mm}\partial\Omega
	\end{equation}
	
	for $\mathcal{L}[u]=-\Delta u(x) + \mathbf{p}(x)\nabla u(x) + q(x)u(x)$, with $\mathbf{p}$ and $q$ given, smooth functions. 
\end{definition}

BVPs like this one are called Dirichlet problems. In what follows, for simplicity, everything will be referred to the case $d=1$. 

\subsection{Finite difference methods}
The discretisations that will be consider in the following are based on the finite difference (FD) approach. The spatial operator $\mathcal{L}$ is evaluated at a set of points $\lbrace x_j\rbrace_{j=1}^{J}$ and the derivatives are replaced with difference quotients of the approximate solution u, which are in turn obtained by truncating (at the desired order of accuracy) the Taylor expansions of the exact solution u(x) around the point $x_j$.\\

Let $\lbrace x_j\rbrace_{j=0}^{J+1}$ be a partition of the interval $(0, L)$, $L\in\mathbb{R}^+$, such that $0=x_0<x_1<\cdots<x_{J+1}=L$. The $j^{th}$ interval will be denoted by $I_j=(x_{j-1}, x_j)$ with mesh size $h_j=|I_j|$. For simplicity, a uniform mesh size will be considered and denoted by $h$, so that $x_j=jh$.\\
Moreover let $u_j$ represents the approximation to the exact solution $u(x_j)$. Then, a simple discrete version of the operator $\Delta$ in \eqref{eqn:BVP} is, for $j=\lbrace 1, \cdots, J\rbrace$, given by:

\begin{align}
	\label{eqn:finite-diff-lap}
	\Delta[u]=\frac{u_{j+1}-2u_j+u_{j-1}}{h^2}
\end{align}

Similarly, the operator $\nabla [u]$ can be discretised as :

\begin{align}
\label{eqn:finite-diff-grad}
\Delta[u]=\frac{u_{j+1}-u_{j-1}}{2h}
\end{align}

Such choices guarantee that the differences between \eqref{eqn:finite-diff-lap} and \eqref{eqn:finite-diff-grad} and their respective continuous counterparts is, at most, $\mathcal{O}(h^2)$, as the following simple calculation explicitly shows. 

\begin{equation}\label{eqn:Taylor-exp}
u(x\pm h)=u(x)\pm hu'(x)+\frac{h^2}{2}u''(x)\pm \frac{h^3}{3!}u'''(x)+\frac{h^4}{4!}u^{(4)}(x)+\mathcal{O}(h^5)
\end{equation}

Inserting Eqn.\eqref{eqn:Taylor-exp} into Eqn.\ref{eqn:finite-diff-lap} and Eqn.\eqref{eqn:finite-diff-grad}, it is easy to see that:

\begin{align}
	\frac{u(x+h)-2u(x)+u(x-h)}{h^2}=u''(x)+\frac{h^2}{12}u^{(4)}(x)+\mathcal{O}(h^4)\\
	\frac{u(x+h)-u(x-h)}{2h}=u'(x)+\frac{h^2}{6}u'''(x)+\mathcal{O}(h^4)
\end{align}

The operators defined in Eqn.\ref{eqn:finite-diff-lap} and Eqn.\eqref{eqn:finite-diff-grad} are \emph{central differences} and will be used for the implementation of the code.\\

The BVP \eqref{eqn:BVP}, can thus be discretised as:

\begin{align}\label{eqn:BVP-discrete}
\begin{split}
-\frac{u_{j+1}-2u_j+u_{j-1}}{h^2}+p_j\frac{u_{j+1}-u_{j-1}}{2h}+q_ju_j=f(x_j), \hspace{5mm}j\in\lbrace 1, \cdots, J\rbrace\\
u_0=v(x_0),\hspace{4mm} u_{J+1}=v(x_{J+1})
\end{split}
\end{align}
	
\subsection{Consistency, convergence and stability of FD methods}
The following discrete function spaces will play an important role in the study of consistency, convergence and stability of FD methods:
\begin{align*}
	&\bar{\mathbb{U}}_h=\lbrace u:\bar{\Omega}_h\rightarrow\mathbb{R}\rbrace\\
	&\bar{\mathbb{U}}^0_h=\lbrace u\in\bar{\mathbb{U}}_h : u|_{\partial\Omega_h}=0\rbrace\\
	&{\mathbb{U}}_h=\lbrace u:\Omega_h\rightarrow\mathbb{R}\rbrace
\end{align*}

where the domain $\bar{\Omega}_h\equiv\lbrace x_j\rbrace_{j=0}^{J+1}$, while $\Omega_h$ represents the set of points in the interior of $\Omega$.\\
It also convenient to introduce the following \emph{restriction operator} $r_h:C(\bar{\Omega})\rightarrow\bar{\mathbb{U}}_h$ defined as:

$$[r_h v]_j=v(x_j),\hspace{5mm}j\in\lbrace 1, \cdots, J\rbrace,\hspace{5mm} v\in C(\Omega)$$

Such an operator allows to compare $C(\bar{\Omega})$ functions with grid functions.\\

Clearly, the discrete problem \eqref{eqn:BVP-discrete} can be now stated as:

\begin{equation}
	\label{eqn:BVP-general-discr}
\end{equation}
$$\bar{\mathcal{L}}_h[u]=f_h$$
where $\mathcal{L}_h:\bar{\mathbb{U}}_h\rightarrow\mathbb{U}_h$ represents both the discretisation of the the different operators involved in Eqn.\eqref{eqn:BVP} and the boundary conditions and $f_h\in\mathbb{U}_h$ is given by $[r_hf]$.\\

\begin{definition}
	\label{defn:consistency}
	The consistency error of the method \eqref{eqn:BVP-general-discr} relative to the exact solution u(x) in a suitable discrete norm is
	$$e_c=||\bar{\mathcal{L}}_h[r_h]-f_h||_{\mathbb{U}_h} $$
	The discretisation is said to be \emph{consistent} if 
	\begin{equation}
		\label{eqn:consistent}
		e_c\rightarrow 0\hspace{3mm}\text{as}\hspace{3mm}h\rightarrow 0
	\end{equation}
	Moreover, the discretisation is said to be consistent of order $p$ if:
	\begin{equation}
	\label{eqn:consistent-p}
	e_c\rightarrow\mathcal{O}(h^p)\hspace{3mm}\text{as}\hspace{3mm}h\rightarrow 0
	\end{equation}
\end{definition}

\begin{definition}
	The discretisation is said to be \emph{convergent} if 
	\begin{equation}
	\label{eqn:convergent}
	||r_h u-u_h||_{{\bar{\mathbb{U}}}_h}\rightarrow 0\hspace{3mm}\text{as}\hspace{3mm}h\rightarrow 0
	\end{equation}
	The discretisation is said to be \emph{convergent} of order p if 
	\begin{equation}
	\label{eqn:convergent-p}
	||r_h u-u_h||_{{\bar{\mathbb{U}}}_h}\rightarrow \mathcal{O}(h^p)\hspace{3mm}\text{as}\hspace{3mm}h\rightarrow 0
	\end{equation}
\end{definition}

\begin{definition}
	The method is \emph{stable} for some constant $C>0$ if
	\begin{equation}
	\label{eqn:stable}
	||v_h w_h||_{{\bar{\mathbb{U}}}_h}\le C||\bar{\mathcal{L}}_h[v_h]-\bar{\mathcal{L}}_h[w_h]||_{{{\mathbb{U}}}_h}\hspace{4mm} \forall v_h,w_h\in\bar{\mathbb{U}}_h
	\end{equation}
\end{definition}

The practical meaning of the previous definitions is as follows: Taylorâ€™s theorem gives a way of estimating the consistency error (given a sufficiently smooth exact solution and suitable discretisation of the right hand side), while stability guarantees that rounding errors occurring in the problem will not have an excessive effect on the final result (see \cite{lec-notes}).

\begin{theorem}
	\label{thm:stab-conv}
	Let $u\in\bar{\mathbb{U}}_h$ solve the discrete problem $\mathcal{L}_h[u]=f_h$. If the method is stable and consistent, then it is convergent. 
\end{theorem}
\begin{proof}
	Stability implies:
	$$||r_hu-u||_{{\bar{\mathbb{U}}}_h}\le C ||\bar{\mathcal{L}}_h[r_hu]-\bar{\mathcal{L}}_h[u]||_{{\bar{\mathbb{U}}}_h}=||\bar{\mathcal{L}}_h[r_hu]-f_h||_{{\bar{\mathbb{U}}}_h}$$
	Hence, by consistency, the method is convergent.
\end{proof}

It is worth noticing that the calculation just presented also shows that the order of convergence is at least equal to the order of consistency.\\

\subsection{The present case}
\label{subsec:the-present-case}
In the present assignment, special cases of an 1D linear advection-diffusion-reaction problem are studied. Such equations describe the advection, diffusion and reaction (sometimes the term absorption is used) of a given quantity represented by $u(x)$: typical example can be picked from hydrodynamics and from physics in general.\\
The problem can be formulated as follows. Given $f\in C([0, L])$, find $u\in C^2(0, L)$ such that:

\begin{equation}
	\label{eqn:ass-prob}
	-u''(x)+ pu'(x)+qu(x)= f,\hspace{5mm}u(0)=a, u(L)=b
\end{equation}

for $a, b, p, q\in\mathbb{R}$. \\

Just as the general method previously introduced, the problem can be represented in the matrix form:

\begin{align}
	\label{eqn:matrix-form}
	A\mathbf{u}=\mathbf{f}
\end{align}

where $\mathbf{u}=(u_1, \cdots, u_J)^{T}$, $\mathbf{f}=(f(x_1),\cdots , f(x_J))^{T}$. In this context, the entries of $A$ are given by:

\begin{align}
	\label{eqn:matrix}
	a_{ij} = \begin{cases}
	-\frac{\alpha}{h^2}-\frac{\beta}{2h}, & \text{for } j=i-1\\
	\frac{\alpha}{h^2}+\gamma, & \text{for } j=i\\
	-\frac{\alpha}{h^2}+\frac{\beta}{2h}, & \text{for } j=i+1\\
	0, &\text{otherwise}
	\end{cases}
\end{align}
	
In this way, differential equations can be solved through the methods used in numerical linear algebra.\\

In what follows, it will be assumed that $f=0$,  $p=\frac{\beta}{\alpha}$, $q=\frac{\gamma}{\alpha}$ (for $\alpha, \beta, \gamma\in\mathbb{R}$), $u(0)=0$ and $u(L)=1$.

\subsubsection{The advection-diffusion problem}
If $\gamma$ is set to $0$ in \eqref{eqn:ass-prob}, a simpler problem, composed of only the advection and diffusion terms, is obtained. The solution to such a problem, for $u(0)=0$ and $u(L)=1$ is easily obtained by means of standard techniques for second order ordinary differential equations and reads:

\begin{align}
	\label{eqn:AD-sol}
	u(x)=\frac{1-e^{\frac{\beta}{\alpha}x}}{1-e^{\frac{\beta}{\alpha}L}},\hspace{5mm} 0\le x\le L
\end{align}

It is clear that the solution can be rewritten as a parametric function of the global P\`{e}clet number $\mathbb{P}_e=\frac{|\beta|L}{2\alpha}$, measures the dominance of the advective term over the diffusive one. Setting $L=1$ as required by the assignment instructions and assuming $\beta>0$ for simplicity, one has:

$$	u(x)=\frac{1-e^{2\mathbb{P}_ex}}{1-e^{2\mathbb{P}_e}}, \hspace{5mm} 0\le x\le 1 $$

In order to have a more clear interpretation of the results presented in the following sections, it is worth studying the limit of both large and small global P\`{e}clet number.\\
If $\frac{\beta}{\alpha}<<1$, a simple Taylor expansion up to first order of \eqref{eqn:AD-sol} yields:

\begin{equation}
	\label{key:AD-smallP}
	u(x)\approx\frac{\frac{\beta}{\alpha}x}{\frac{\beta}{\alpha}}=x
\end{equation}

The dominance of $\alpha$ with respect to $\beta$ makes the solution closer to the one of the problem $\alpha u''(x)=0$ with the same boundary conditions (BC). On the other hand, for $\frac{\beta}{\alpha}>>1$ one has:

\begin{equation}
\label{key:AD-bigP}
u(x)\approx\frac{\frac{\beta}{\alpha}x}{\frac{\beta}{\alpha}}=e^{\frac{\beta}{\alpha}(x-1)}=e^{-\frac{\beta}{\alpha}(1-x)}
\end{equation}

Since the exponent is big and negative the solution is almost equal to zero everywhere unless a small neighbourhood of the point $x=1$ where the term $ 1-x $ becomes very small and the solution joins the value $1$ with an exponential behaviour. The width of the neighbourhood is of the order of $\frac{\alpha}{\beta}$ and thus it is quite small: in such an event, we say that the solution exhibits a boundary layer of width $\mathcal{O}\left(\frac{\alpha}{\beta}\right)$ at $x=1$ (see Ref.\cite{numerical-math}).

\subsubsection{The diffusion-reaction problem}
On the other hand, if $\beta$ is set to zero, the solution to the corresponding problem (for the same BC) is given by:

\begin{equation}
	\label{eqn:DR-sol}
	u(x)=\frac{\sinh(\frac{\gamma}{\alpha}x)}{\sinh(\frac{\gamma}{\alpha}L)},\hspace{5mm} 0\le x\le L
\end{equation}

In this case, the relevant parameter to study the dominance of the advection with respect to the response terms, is represented by $D_a=\frac{\gamma}{\alpha}$ and is called Damk\"{o}hler number.\\

\section{The program}
\subsection{Implementation}
The program is built on the class ADR , which stands for Advection-Diffusion-Reaction. The class has 7 private variables: $J$ (the number of points in the interior of $[0,L]$), $\alpha$, $\beta$, $\gamma$ (the parameters associated to the ADR equation at hand), $L$ (the length of the interval), $u_0$ (the boundary condition at $0$) and $u_L$ (boundary condition at $L$). In this way, once the constructor is called, all the variables and parameters defining problem \eqref{eqn:ass-prob} are set.\\
As pointed out in $\S$(\ref{subsec:the-present-case}), BVPs can be solved through numerical methods for linear algebra: the member function \emph{MatrixBuild}, which accepts no argument, constructs matrix $A$ of Eqn.\eqref{eqn:matrix-form}, whose entries are given by \eqref{eqn:matrix}, which will be later inverted through the Gauss-Seidel algorithm. Such a construction takes places within the function \emph{Solver}, that performs the tests required by the assignment instructions through the Gauss-Seidel method and outputs the results on different .txt files. For clarity reasons,  each time a simulation is performed, a suitable success message containing the different parameters of the corresponding test, is printed on the terminal. \emph{Solver} accepts 11 arguments: 7 of them are simply represented by the 7 private variables of the ADR class, while the remaining 4 are the ones needed by the Gauss-Seidel algorithm to invert matrix $A$. In particular, \emph{u$\_$x} represents the final solution of the equation, \emph{itCheck} the number of iterations after which a check for stagnation of the method is performed, \emph{MaxIter} the maximum number of iterations the user is willing ''wait" and \emph{tol} the given tolerance.\\ 
The function \emph{An$\_$sol} accepts 7 input variables, representing the 7 parameters needed to define the ADR equation at hand, and uses them to compute the analytical solutions at each point of $\Omega_h$. \emph{An$\_$sol} simply computes either Eqn.\eqref{eqn:AD-sol} or Eqn.\eqref{eqn:DR-sol} at the given set of points: the choice between the two solutions is made by means of an \emph{if} instruction, checking if $\alpha\neq 0, \gamma=0$ or if $\alpha\neq 0, \beta=0$. When none of the above conditions is satisfied, the program exits with an appropriate error message to the user.\\
Discretised and the analytical solution are finally compared within the function \emph{ADR$\_$Test}. Such a function loops over different of values of $J$ and prints the final error between the numerical solution and the analytical one, in the $L_\infty$ norm, on suitable .txt files, whose names are defined through the parameters the user has set for the test. The program is thus particularly flexible, since, by means of simply changing the values provided in the main file, both class of equations can be tested, with no further modifications to the code.\\ 


\subsection{Solving the equation}
The core of the whole program is clearly represented by the function performing the Gauss-Seidel algorithm. Such a function has been chosen to be a member function and has been built to accept 7 parameters: the initial guess $x_0$, the vector to invert against $b$, the tolerance required by the method (\emph{tol}), the number of iterations after which a check for stagnation has to be made (\emph{itCheck}), two files on which the residual error at every iteration and the solution are printed respectively, the maximum number of iteration the user is willing to wait for the algorithm to converge (\emph{MaxIter}).\\
Looking at Eqn. \eqref{eqn:Gauss-Seidel}, it is clear that the elements of $x_0$ can be overwritten as they are computed in the algorithm and only one storage vector is needed: $x_0$ has then been passed by reference to the function, so that no copy of it has to be produced.\\
It is worth pointing out that the matrix entries needed for overwriting the elements of $x_0$ are obtained through the member function $getValue$.\footnote{More comments on this choice can be found in $\S\ref{subsec:perf}$}.\\  
In case the matrix is not a square matrix and the sizes of $x_0$ and $b$ do not equal the row and column size of the matrix, the program exits with an appropriate warning to the user. If this is not the case, the iterations start. The program runs until either the residual error becomes smaller than the given tolerance or the maximum number of iterations \emph{MaxIter} has been reached. In order to make sure that the program has not stagnated, every \emph{itCheck} iterations, the current residual error, in the $L_\infty$ norm, is compared with the one computed and stored \emph{itCheck} iterations before: if the residual has not decreased, the program exits with an appropriate stagnation warning to the user. An appropriate warning is printed on the terminal if the program exist due to having reached the maximum number of iterations the user has fixed.\\
The sudo code for the implementation of the algorithm can be found either in Ref.\cite{lec-notes} or on Wikipedia and will not be reported here. 

\subsection{Testing correctness}
In order to check the correctness of the implementation of the Gauss-Seidel algorithm, the tests requested by the assignment have been performed: for this reason, a non-member function Gauss$\_$Seidel$\_$test has been built. Such a function takes 6 arguments: the size of the matrix $A$ that has to be initialised to perform the test, the parameters $\delta$ and $\lambda$ necessary to the implementation of the same tests, the fixed tolerance (\emph{tol}), the number of iterations after which a check for stagnation has to be made (\emph{itCheck}), the maximum number of iteration the user is willing to wait for the algorithm to converge (\emph{MaxIter}). The last 3 parameters will be used by the member function Gauss$\_$Seidel previously introduced, within the function Gauss$\_$Seidel$\_$test.\\
The vectors $x_0$, $b$, $\omega$ and $D$ and the parameter $a=4(\delta-1)$ necessary to construct the matrix $A$ are initialised by this function. The entries of $A$ are added by means of the member function \emph{addEntry}.\\
The files on which the residual error at every iteration and the corresponding final solution are printed, are identified by the value of $\delta$, the value of $\lambda$, by the strings \emph{Residual} or \emph{solution} and the size of the matrix with on the output files.\\
The function prints on the terminal the error between the exact and the found solution, indexing it through the value of $\lambda$, $\delta$ and the size $N$ of matrix $A$.\\

\subsection{Running the program}
The program is run through the Makefile provided. The current optimisation is set to -Ofast and all following tests have been performed with this optimisation choice. Every possible error or warning has been explicitly checked by means of the instruction \emph{-Wall -Wfatal-errors -pedantic} (that can still be found commented out in the Makefile) before proceeding to performing the different tests: no warnings appeared.\\
The program generates a fixed number of output files, in which the results of the tests have been stored. Some of these files are used to produce plots by means of two different plotscripts.\\


\subsection{Results of the tests}
In the following, the results of the more significant tests performed will be reported and briefly discussed. The tolerance has been set to $10^{-6}$ for each of the tests.

\subsubsection{Varying the matrix size}
The Gauss$\_$Seidel$\_$test function has been shown to work for the case of $N=100$, $N=1000$ and $N=10000$, N being the size of the matrix, as required by the assignment.
The time $T_{10000}$ required to reach convergence in the case $N=10000$ has been explicitly computed trough the following instructions:

\begin{verbatim}
	auto start = std::chrono::high_resolution_clock::now();
	
	auto finish = std::chrono::high_resolution_clock::now();
	std::chrono::duration<double> elapsed = finish - start;
	std::cout<< "Elapsed time: " << elapsed.count() << "s\n";
\end{verbatim} 

and found to be $T_{10000}\approx 83$ seconds. The algorithm has thus been shown to work in quite a short time, so that the program can be regarded as being reasonably efficient.\footnote{More comments on the efficiency of the code can be found in $\S\ref{subsec:perf}$.} The solutions $x_0$ obtained for the three different cases can be found in the files \emph{d$\_$1.000000$\_$l$\_$0.000000$\_$GSsolution$\_$100.txt}, \emph{d$\_$1.000000$\_$l$\_$0.000000$\_$GSsolution$\_$1000.txt} and \emph{d$\_$1.000000$\_$l$\_$0.000000$\_$GSsolution$\_$10000.txt}. The residual error at every iteration has been printed in corresponding \emph{Residual} files.\\
Finally, the distance between the exact solution and the approximate one is printed on the terminal. Unsurprisingly, the error becomes bigger as the size of the matrix grows.\\

\subsubsection{Varying $\delta$}
The performance of the algorithm has been tested for different values of $\delta$, as required by the assignment. Having shown that the algorithm converges in a reasonably short time even in the case of $N=10000$, the size of the matrix for this series of tests has been fixed to $N=100$.\\
In order to investigate the change in performance for different order of magnitudes, 10 different values of $\delta$, ranging from $1.0$ to $10^5$ have been used.



It is clear that the desired closeness to the exact solution, measured in terms of the residual error, is reached independently of how large the initial residual is. As expected, the residual error is a monotone decreasing function of the iterations.\\
For $\delta\ge50$, a concave region in the residual error profile is noticeable, signalling a change in the $``$velocity$"$ of convergence. Interestingly, such a region is particularly steep for $\delta=500$ and an overlap with the curves associated with smaller $\delta$ is present.\\

Clearly, the number of iterations required to have a residual error smaller than the fixed tolerance grows with $\delta$, i.e. the code becomes less efficient when $\delta$ grows. In other words, the spectral radius of the matrix $P^{-1}N$ (see $\S\ref{subsubsec:gauss-seidel-method}$) gets smaller for increasing $\delta$.\\

\subsubsection{Varying $\lambda$}
The performance of the algorithm has been tested for different values of $\lambda$, as required by the assignment. Having shown that the algorithm converges in a reasonably short time even in the case of $N=10000$, the size of the matrix for this series of tests has been fixed to $N=100$.\\
In order to investigate the change in performance for different order of magnitudes, 10 different values of $\lambda$, ranging from $1.0$ to $10^5$ have been used.



It is clear that the presence of $\lambda$ changes the efficiency of the algorithm. In particular, the plot shows that for $\lambda\ge10^3$, the desired convergence is reached in just two iterations.\\
This can be understood by recalling what has been mentioned in $\S\ref{subsubsec:splitting-methods}$ and $\S\ref{subsubsec:gauss-seidel-method}$ about the decomposition of A into $P_G=D-E$ and $N_G=F$. The presence of $\lambda\in\mathbb{R}^{+}$ modifies the diagonal component of $A$ and consequently the matrix $D$. The preconditioner matrix $P$ thereby obtained is different from the one of the initial problem: as a matter of fact, for $\lambda\in\mathbb{R}^{+}$ the matrix $A$ becomes strictly diagonal dominant. Differently from the case of $\delta$, the spectral radius of $P^{-1}N$ becomes smaller and a faster convergence is obtained.\\





\section{Conclusive remarks}
In this section some additional comments and observation are presented.

\subsection{Memory}
When dynamically allocating memory, one has always to make sure to free the reserved locations as soon as they are no longer needed. With the help of the software \emph{valgrind}, possible memory leaks have been checked. To do this, is it necessary to compile with \emph{-g} and \emph{-O1} optimisation. After having compiled the program, the following instruction has been used:

\begin{verbatim}
$ valgrind --leak-check=yes ./sparsematrix
\end{verbatim}

The following output has been produced on the terminal:

\begin{verbatim}
==4200== Memcheck, a memory error detector
==4200== Copyright (C) 2002-2017, and GNU GPL'd, by Julian Seward et al.
==4200== Using Valgrind-3.13.0 and LibVEX; rerun with -h for copyright info
==4200== Command: ./sparsematrix
==4200== 
==4200== 
==4200== HEAP SUMMARY:
==4200==     in use at exit: 0 bytes in 0 blocks
==4200==   total heap usage: 1,252,428 allocs, 1,252,428 frees, 989,616,540 bytes allocated
==4200== 
==4200== All heap blocks were freed -- no leaks are possible
==4200== 
==4200== For counts of detected and suppressed errors, rerun with: -v
==4200== ERROR SUMMARY: 0 errors from 0 contexts (suppressed: 0 from 0)
\end{verbatim}

One can clearly see that no memory has been lost, so that everything has been deleted correctly. \\
It may seem a little strange at fist that all the heap blocks were freed, even though no explicit call to the destructor has been made. However, all instances are built by the function Gauss$\_$Seidel$\_$test, so that the deletion is automatically performed every time the programs exits the function.\\
The absence of memory leaks is however non trivial, since it proves that the destructor has been constructed correctly. This can be explicitly checked by means of commenting out the last two lines of the destructor: if \emph{valgrind} is called again, some memory leak will be found.\\ 


\subsection{Performances}
\label{subsec:perf}
In order to test performance, the following instructions have been used\footnote{The test has been performed only on the part of the code concerned with the test over different $\lambda$ and $\delta$, commenting out the part on different $N$ in the main file. }:
\begin{verbatim}
$ valgrind --tool=callgrind ./sparsematrix
$ kcachegrind
\end{verbatim}

The first command analyses the perfomances in the terms of load distrubution, while \emph{kcachegrind} is used to visualise the load distribution results. Results are reported in the following screenshot.


It is clear that a better efficiency could have been obtained if the program had not relied so much on the \emph{getValue} function, which is not particularly efficient.\\

\subsubsection{Possible solutions}
A possible way to optimise the code is to sort the indexes and values as soon as they are entered, so that each index can be associated to the corresponding value in a unique way and without the necessity to call a the \emph{getValue}. Such a solution, however, has not been implemented, since even though it is clearly more efficient for the matrix given in the assignment, it is not immediately clear how faster and optimised the code could become in the case of denser sparse matrix. On the other hand, it is still reasonable to expect that such a choice will make the program faster, since the sorting takes place only at the beginning of the program and \emph{getValue} is called a large number of times during the iterations.\\
Possibly, the best solution is to store the values and the indexes by means of the \emph{insert} function instead that through a \emph{push$\_$back}: this way, the values
 will be automatically sorted.\\



\cleardoublepage
%\add1contentsline{toc}{chapter}{\bibname}
\begin{thebibliography}{99}

\bibitem{numerical-math} A. Quateroni, R. Sacco, F. Saleri;
\emph{Numerical Mathematics}, Vol.37, Springer Verlag, (2007).

\bibitem{lec-notes} T. Grafke;
\emph{Scientific Computing}, Lecture Notes, University of Warwick, (2018).







\printindex
\end{thebibliography}
\bibliography{bibliography} % BibTeX database without .bib extension
\end{document}



%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\bibliographystyle{apalike}

%\bibliography{sample}

----------------------------------------------------------------------------------------


%end{document}